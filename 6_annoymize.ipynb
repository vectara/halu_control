{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rogger/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, pipeline, AutoModelForCausalLM\n",
    "from transformers import LogitsProcessor\n",
    "from typing import Iterable\n",
    "import envs\n",
    "import pandas as pd\n",
    "import string\n",
    "from leaderboard import SummaryGenerator, EvaluationModel, run_eval, run_eval_TT\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-25 17:46:55,069 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.20s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME,\n",
    "                                             device_map=\"auto\",\n",
    "                                             torch_dtype=\"auto\",\n",
    "                                             attn_implementation=\"flash_attention_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.chat_template = open(\"mistral_template.jinja\", \"r\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_func(source, debug=False):\n",
    "    messages = [{\"role\": \"system\", \"content\": envs.SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": envs.USER_PROMPT.format(passage=source)}]\n",
    "    input_ids = tokenizer.apply_chat_template(messages, \n",
    "                                              add_generation_prompt=True, \n",
    "                                              return_tensors=\"pt\").to(\"cuda\")\n",
    "    out = model.generate(input_ids, \n",
    "                         do_sample=False, \n",
    "                         max_new_tokens=512,\n",
    "                         pad_token_id=tokenizer.eos_token_id)\n",
    "    text = tokenizer.decode(out[0][len(input_ids[0]):], skip_special_tokens=True)\n",
    "    if debug:\n",
    "        print(tokenizer.decode(out[0], skip_special_tokens=False))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] You are a chat bot answering questions using data. You must stick to the answers provided solely by the text in the passage provided. \n",
      "\n",
      "You are asked the question 'Provide a concise summary of the following passage, covering the core pieces of information described': \n",
      "Passage:\n",
      "The first vaccine for [CONDITION_1] was approved by the [ORGANIZATION_1] in [DATE_INTERVAL_1] in [LOCATION_COUNTRY_1], [DURATION_1] after the initial outbreak in [DATE_INTERVAL_2]. To produce the vaccine, scientists had to sequence the DNA of [CONDITION_1], then identify possible vaccines, and finally show successful clinical trials. Scientists say a vaccine for [CONDITION_2] is unlikely to be ready this year, although clinical trials have already started. [/INST] The first vaccine for [CONDITION_1] was approved by [ORGANIZATION_1] in [LOCATION_COUNTRY_1] in [DATE_INTERVAL_1] after the initial outbreak in [DATE_INTERVAL_2]. The vaccine was produced by sequencing the DNA of [CONDITION_1], identifying possible vaccines, and successfully completing clinical trials. A vaccine for [CONDITION_2] is currently in clinical trials but scientists do not expect it to be ready this year.</s>\n"
     ]
    }
   ],
   "source": [
    "text = gen_func(\"The first vaccine for [CONDITION_1] was approved by the [ORGANIZATION_1] in [DATE_INTERVAL_1] in [LOCATION_COUNTRY_1], [DURATION_1] after the initial outbreak in [DATE_INTERVAL_2]. To produce the vaccine, scientists had to sequence the DNA of [CONDITION_1], then identify possible vaccines, and finally show successful clinical trials. Scientists say a vaccine for [CONDITION_2] is unlikely to be ready this year, although clinical trials have already started.\", debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ = SummaryGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 477/1006 [29:51<56:02,  6.36s/it]  "
     ]
    }
   ],
   "source": [
    "df = summ.generate_summaries(pd.read_csv(\"anonymized_leaderboard.csv\"), gen_func)\n",
    "df.to_csv(\"generated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# De-anonymize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_sum = pd.read_csv(\"generated.csv\")\n",
    "ano_src = pd.read_csv(\"anonymized_leaderboard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dea_sum = []\n",
    "for ano_sum, ent_info in zip(gen_sum[\"summary\"], ano_src[\"entities\"]):\n",
    "    ent_info = json.loads(ent_info)\n",
    "    for entity in ent_info:\n",
    "        ano_sum = ano_sum.replace(f\"[{entity['processed_text']}]\", entity[\"text\"])\n",
    "    dea_sum.append(ano_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldb = pd.read_csv(\"leaderboard_dataset.csv\")\n",
    "ldb[\"summary\"] = dea_sum\n",
    "ldb = ldb.rename(columns={\"text\":\"source\"})\n",
    "ldb.to_csv(\"generated_deanonymized.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_eval_TT(\"generated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
