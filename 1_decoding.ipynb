{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rogger/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, pipeline, AutoModelForCausalLM\n",
    "from transformers import LogitsProcessor\n",
    "from typing import Iterable\n",
    "import envs\n",
    "import pandas as pd\n",
    "import string\n",
    "from leaderboard import SummaryGenerator, EvaluationModel, run_eval\n",
    "\n",
    "# MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "MODEL_NAME = \"mistral_dpo_5k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 22:31:03,002 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.34it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME,\n",
    "                                             device_map=\"auto\",\n",
    "                                             torch_dtype=\"auto\",\n",
    "                                             attn_implementation=\"flash_attention_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.chat_template = open(\"mistral_template.jinja\", \"r\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhiteListLogitsProcessor(LogitsProcessor):\n",
    "    \"\"\"\n",
    "    A simple LogitsProcessor constraining the generation to \"white list\", i.e. a set of allowed tokens.     \n",
    "    \"\"\"\n",
    "    def __init__(self, white_list_word_ids: Iterable[int]):\n",
    "        self.white_list = white_list_word_ids \n",
    "        self.mask = None \n",
    "            \n",
    "    def __call__(self, input_ids, scores):\n",
    "        \"\"\"\n",
    "        This method will be called during each step of the beam search algorithm. \n",
    "        The method takes as input the input_ids sequence of the partially generated beam and the scores of the next possible tokens.\n",
    "        By manipulating these scores based on the tokens present in the input_ids, we can control the structure of the generated sentence.\n",
    "        \"\"\"\n",
    "        if self.mask is None:\n",
    "            self.mask = torch.ones(scores.shape).to(scores.device)\n",
    "            # put zeros in allowed tokens\n",
    "            self.mask[:, self.white_list] = 0\n",
    "            self.mask = self.mask.bool()\n",
    "        scores = scores.masked_fill(self.mask, -float(\"inf\"))\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good_words = \"only these words can occur in the generated text\"\n",
    "# good_word_ids = tokenizer.encode(good_words)\n",
    "# white_list_processor = WhiteListLogitsProcessor(good_word_ids)\n",
    "# input_seq = \"here are the input words to condition generated text upon\"\n",
    "# input_ids = tokenizer.encode(input_seq, return_tensors='pt').to(\"cuda\")\n",
    "# out = model.generate(input_ids, do_sample=False, logits_processor=[white_list_processor])\n",
    "# print(tokenizer.batch_decode(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_func(source, debug=False):\n",
    "    messages = [{\"role\": \"system\", \"content\": envs.SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": envs.USER_PROMPT.format(passage=source)}]\n",
    "    # good_word_ids = tokenizer.encode(source + \"\\n\" + \"\\n\".join(string.printable) + \"\\n\" + tokenizer.eos_token)\n",
    "    # white_list_processor = WhiteListLogitsProcessor(good_word_ids)\n",
    "    input_ids = tokenizer.apply_chat_template(messages, \n",
    "                                              add_generation_prompt=True, \n",
    "                                              return_tensors=\"pt\").to(\"cuda\")\n",
    "    out = model.generate(input_ids, \n",
    "                         do_sample=False, \n",
    "                        #  logits_processor=[white_list_processor],\n",
    "                         max_new_tokens=512,\n",
    "                        #  num_beams=10,\n",
    "                         pad_token_id=tokenizer.eos_token_id)\n",
    "    text = tokenizer.decode(out[0][len(input_ids[0]):], skip_special_tokens=True)\n",
    "    if debug:\n",
    "        print(tokenizer.decode(out[0], skip_special_tokens=False))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] You are a chat bot answering questions using data. You must stick to the answers provided solely by the text in the passage provided. \n",
      "\n",
      "You are asked the question 'Provide a concise summary of the following passage, covering the core pieces of information described': \n",
      "Passage:\n",
      "The first vaccine for Ebola was approved by the FDA in 2019 in the US, five years after the initial outbreak in 2014. To produce the vaccine, scientists had to sequence the DNA of Ebola, then identify possible vaccines, and finally show successful clinical trials. Scientists say a vaccine for COVID-19 is unlikely to be ready this year, although clinical trials have already started. [/INST] The first Ebola vaccine was approved by the FDA in the US in 2019, five years after the initial outbreak in 2014. Scientists had to sequence the DNA of Ebola, identify possible vaccines, and conduct successful clinical trials to produce the vaccine. A vaccine for COVID-19 is unlikely to be ready this year, although clinical trials have already started.</s>\n"
     ]
    }
   ],
   "source": [
    "text = gen_func(\"The first vaccine for Ebola was approved by the FDA in 2019 in the US, five years after the initial outbreak in 2014. To produce the vaccine, scientists had to sequence the DNA of Ebola, then identify possible vaccines, and finally show successful clinical trials. Scientists say a vaccine for COVID-19 is unlikely to be ready this year, although clinical trials have already started.\", debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ = SummaryGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1006/1006 [1:19:11<00:00,  4.72s/it]\n"
     ]
    }
   ],
   "source": [
    "df = summ.generate_summaries(pd.read_csv(\"leaderboard_dataset.csv\"), gen_func)\n",
    "df.to_csv(\"generated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rogger/miniconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/rogger/miniconda3/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "2024-06-13 23:56:42,701 - INFO - Use pytorch device: cuda\n",
      "Evaluating hallucinations: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1006/1006 [00:14<00:00, 67.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Length 96.98508946322067\n",
      "Answer Rate 1.0\n",
      "Consistent Rate 95.82504970178927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_eval(\"generated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
