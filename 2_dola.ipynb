{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rogger/miniconda3/envs/DoLa/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.28.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import transformers\n",
    "print(transformers.__version__)\n",
    "from transformers import AutoTokenizer, pipeline, AutoModelForCausalLM\n",
    "from typing import Iterable\n",
    "import envs\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import DoLa.dola as dola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 12.48it/s]\n"
     ]
    }
   ],
   "source": [
    "model = dola.DoLa(MODEL_NAME, \"cuda\", num_gpus=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Where is the capital of Washington State?\"\n",
    "layers = [16,18,20,22,24,26,28,30,32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL OUTPUT: \n",
      "\n",
      "\n",
      "The capital city of Washington State is Olympia.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('\\n\\nThe capital city of Washington State is Olympia.',\n",
       " {16: 12, 18: 0, 20: 1, 22: 0, 24: 0, 26: 0, 28: 0, 30: 0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(input_text,\n",
    "               temperature=0.7,\n",
    "               mature_layer=layers[-1],\n",
    "               candidate_premature_layers=layers[:-1],\n",
    "               mode=\"dola\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_func(source, debug=False):\n",
    "    input_text = \"<s> [INST] \" + envs.SYSTEM_PROMPT + \" \\n\\n\" + envs.USER_PROMPT.format(passage=source) + \" [/INST]\"\n",
    "    if debug: print(input_text)\n",
    "    out = model.generate(input_text,\n",
    "                         do_sample=False,\n",
    "                         max_new_tokens=512,\n",
    "                         pad_token_id=model.tokenizer.eos_token_id,\n",
    "                         mature_layer=layers[-1],\n",
    "                         candidate_premature_layers=layers[:-1],\n",
    "                         mode=\"dola\",\n",
    "                         verbose=False)\n",
    "    return out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] You are a chat bot answering questions using data. You must stick to the answers provided solely by the text in the passage provided. \n",
      "\n",
      "You are asked the question 'Provide a concise summary of the following passage, covering the core pieces of information described': \n",
      "Passage:\n",
      "The first vaccine for Ebola was approved by the FDA in 2019 in the US, five years after the initial outbreak in 2014. To produce the vaccine, scientists had to sequence the DNA of Ebola, then identify possible vaccines, and finally show successful clinical trials. Scientists say a vaccine for COVID-19 is unlikely to be ready this year, although clinical trials have already started. [/INST]\n",
      "The first Ebola vaccine was approved by the FDA in the US in 2019, five years after the initial outbreak in 2014. Scientists had to sequence the DNA of Ebola, identify possible vaccines, and conduct successful clinical trials to produce the vaccine. A vaccine for COVID-19 is unlikely to be ready this year, despite clinical trials already starting.\n"
     ]
    }
   ],
   "source": [
    "text = gen_func(\"The first vaccine for Ebola was approved by the FDA in 2019 in the US, five years after the initial outbreak in 2014. To produce the vaccine, scientists had to sequence the DNA of Ebola, then identify possible vaccines, and finally show successful clinical trials. Scientists say a vaccine for COVID-19 is unlikely to be ready this year, although clinical trials have already started.\", debug=True)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from leaderboard import SummaryGenerator, EvaluationModel\n",
    "summ = SummaryGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1006 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1006/1006 [1:24:16<00:00,  5.03s/it]\n"
     ]
    }
   ],
   "source": [
    "df = summ.generate_summaries(pd.read_csv(\"leaderboard_dataset.csv\"), gen_func)\n",
    "df.to_csv(\"generated.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
